<!DOCTYPE html><html lang="zh-cn" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>多臂老虎机 | 千里之行，始于足下</title><meta name="author" content="一瓢清浅"><meta name="copyright" content="一瓢清浅"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="本文深入浅出讲解多臂老虎机（Multi-Armed Bandit, MAB）问题及其常见变体，配合准确的数学公式和直观理解，适合初学者入门强化学习与序贯决策理论。">
<meta property="og:type" content="article">
<meta property="og:title" content="多臂老虎机">
<meta property="og:url" content="https://jiliguluss.github.io/2025/%E5%A4%9A%E8%87%82%E8%80%81%E8%99%8E%E6%9C%BA/index.html">
<meta property="og:site_name" content="千里之行，始于足下">
<meta property="og:description" content="本文深入浅出讲解多臂老虎机（Multi-Armed Bandit, MAB）问题及其常见变体，配合准确的数学公式和直观理解，适合初学者入门强化学习与序贯决策理论。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://jiliguluss.github.io/img/photo.jpg">
<meta property="article:published_time" content="2025-05-28T00:57:38.000Z">
<meta property="article:modified_time" content="2025-06-04T08:48:59.531Z">
<meta property="article:author" content="一瓢清浅">
<meta property="article:tag" content="MAB">
<meta property="article:tag" content="UCB">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://jiliguluss.github.io/img/photo.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "多臂老虎机",
  "url": "https://jiliguluss.github.io/2025/%E5%A4%9A%E8%87%82%E8%80%81%E8%99%8E%E6%9C%BA/",
  "image": "https://jiliguluss.github.io/img/photo.jpg",
  "datePublished": "2025-05-28T00:57:38.000Z",
  "dateModified": "2025-06-04T08:48:59.531Z",
  "author": [
    {
      "@type": "Person",
      "name": "一瓢清浅",
      "url": "https://jiliguluss.github.io/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/photo.jpg"><link rel="canonical" href="https://jiliguluss.github.io/2025/%E5%A4%9A%E8%87%82%E8%80%81%E8%99%8E%E6%9C%BA/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '多臂老虎机',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg" style="background-image: url(/img/cover.png);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/photo.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"></div><div class="site-data text-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">34</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">37</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">18</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">千里之行，始于足下</span></a><a class="nav-page-title" href="/"><span class="site-name">多臂老虎机</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">多臂老虎机</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-05-28T00:57:38.000Z" title="Created 2025-05-28 08:57:38">2025-05-28</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-06-04T08:48:59.531Z" title="Updated 2025-06-04 16:48:59">2025-06-04</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%AE%97%E6%B3%95/">算法</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%AE%97%E6%B3%95/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word Count:</span><span class="word-count">1.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading Time:</span><span>7mins</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id data-flag-title><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="container post-content" id="article-container"><h2 id="一、多臂老虎机问题"><a href="# 一、多臂老虎机问题" class="headerlink" title="一、多臂老虎机问题"></a>一、多臂老虎机问题 </h2><p> 假设在一家赌场有多台老虎机，每台机器拉一下手柄都会随机给出一定的金币作为奖励，但不同机器的回报率各不相同且事先未知。怎样在有限的尝试次数内，尽可能多地赢取金币，就是经典的 <strong> 多臂老虎机（Multi‐armed Bandit, MAB）问题</strong>。</p>
<p>多臂老虎机的玩家既要“探索”未知臂以便发现更高的期望收益，也要“利用”目前已知的优臂以获取更多回报。所以多臂老虎机问题的核心在于：如何 <strong> 在探索和利用之间做出最优平衡</strong>（exploration–exploitation trade-off），以最大化累积回报或最小化累积遗憾。</p>
<h3 id="（一）数学建模"><a href="#（一）数学建模" class="headerlink" title="（一）数学建模"></a>（一）数学建模 </h3><p> 设有 $K$ 个臂，每个臂 $i$ 对应一个未知概率分布 $P_i$，奖励取值范围设为 $[0,1]$（可以归一化），记：</p>
<ul>
<li>$X_{i,t} \sim P_i$：第 $t$ 次选择臂 $i$ 获得的奖励；</li>
<li>$\mu_i = \mathbb{E}[X_{i,t}]$：臂 $i$ 的期望奖励（固定但未知）；</li>
<li>$\mu^* = \max_{1 \le i \le K} \mu_i$：最优臂的期望奖励；</li>
<li>$A_t$：第 $t$ 轮选择的臂编号；</li>
<li>$T$：总轮数（时间步数）；</li>
</ul>
<p>目标是设计策略 $\pi$ 来最小化 <strong> 累积遗憾（Cumulative Regret）</strong>：</p>
<script type="math/tex; mode=display">
Rt(T) = T\mu^* - \mathbb{E} \left[\sum_{t=1}^T X_{A_t,t} \right] = \sum_{i=1}^T \Delta_i \cdot \mathbb{E}[N_i(T)] \quad \text{(越小越好)}</script><p>其中：</p>
<ul>
<li>$\Delta_i = \mu^* - \mu_i$ 是臂 $i$ 的“劣势”；</li>
<li>$N_i(T) = \sum_{t=1}^T \mathbb{I}\{A_t = i\}$ 是臂 $i$ 被选择的次数；</li>
</ul>
<p>这个遗憾函数反映了我们策略由于没有总是选择最优臂而损失的期望奖励。</p>
<p>由于 $T\mu^{\ast}$ 是个固定值，最小化累计遗憾也等价于最大化 <strong> 累计奖励（Cumulative Reward）</strong>：</p>
<script type="math/tex; mode=display">
Rd(T) = \mathbb{E}\left[\sum_{t=1}^T X_{A_t,t}\right] \quad \text{(越大越好)}</script><h3 id="（二）求解算法"><a href="#（二）求解算法" class="headerlink" title="（二）求解算法"></a>（二）求解算法 </h3><h4 id="1-varepsilon- 贪心算法（-varepsilon-Greedy）"><a href="#1-varepsilon- 贪心算法（-varepsilon-Greedy）" class="headerlink" title="1. $\varepsilon$- 贪心算法（$\varepsilon$-Greedy）"></a>1. $\varepsilon$- 贪心算法（$\varepsilon$-Greedy）</h4><p> 设 $\hat{\mu}_i(t)$ 为到第 $t$ 轮为止臂 $i$ 的平均奖励估计：</p>
<script type="math/tex; mode=display">
\hat{\mu}_i(t) = \frac{1}{N_i(t)} \sum_{s=1}^{t} X_{i,s} \cdot \mathbb{I}\{A_s = i\}</script><p>其中：</p>
<ul>
<li>$N_i(t)$：到第 $t$ 轮为止臂 $i$ 被选择的次数；</li>
<li>$X_{i,s}$：第 $s$ 轮从臂 $i$ 获得的奖励；</li>
<li>$\mathbb{I}\{A_s = i\}$ 是指示函数，表示第 $s$ 轮是否选择了臂 $i$。</li>
</ul>
<p>$\varepsilon$-Greedy 算法是求解 MAB 问题最简单的策略：</p>
<ul>
<li>以概率 $1 - \varepsilon$ 选择当前平均奖励最高的臂（利用）；</li>
<li>以概率 $\varepsilon$ 随机选择一个臂（探索）。</li>
</ul>
<p>$\varepsilon$-Greedy 算法的缺点为：固定的 $\varepsilon$ 会导致 <strong> 线性遗憾</strong>。随着时间推移，臂的奖励大小会越来越明确，此时应该以利用为主，减少无谓探索。固定的 $\varepsilon$ 会始终浪费一定时间进程试错，即策略中“乱选”的部分不会随着学习而减少。</p>
<p>$\varepsilon$-Greedy 算法的改进方法为：随时间减小 $\varepsilon$，例如设置：</p>
<script type="math/tex; mode=display">
\varepsilon_t = \frac{1}{t} \quad \text{或} \quad \varepsilon_t = \frac{1}{\sqrt{t}}</script><p>这表示初期我们多探索（$\varepsilon_t$ 较大）；随着时间增长，我们更倾向于利用已知信息（$\varepsilon_t$ 趋近于 0）；这样策略越来越稳定，最终能逼近最优臂，获得 <strong> 次线性遗憾</strong>。</p>
<h4 id="2- 上置信界算法（UCB1）"><a href="#2- 上置信界算法（UCB1）" class="headerlink" title="2. 上置信界算法（UCB1）"></a>2. 上置信界算法（UCB1）</h4><p>UCB1 的核心思想是：利用置信区间（confidence bound）做乐观估计，优先尝试“可能好”的臂。</p>
<p>UCB1 通过计算 <strong>当前平均奖励 + 一个鼓励探索的项（不确定性）</strong> 来估计每个臂的乐观表现，并挑选最乐观的臂：</p>
<script type="math/tex; mode=display">
A_t = \arg\max_{1 \le i \le K} \left(\hat{\mu}_i(t-1) + \sqrt{\frac{2 \ln t}{N_i(t-1)} } \right)</script><p>其中：</p>
<ul>
<li>$\hat{\mu}_i(t-1)$：第 $t-1$ 步时臂 $i$ 的平均奖励；</li>
<li>$N_i(t-1)$：到 $t-1$ 步为止，臂 $i$ 被选择的次数，臂被选中的次数越多，臂的奖励的不确定性越小，探索项越小；</li>
<li>$\sqrt{\frac{2 \ln t}{N_i(t-1)} }$ 是 <strong> 置信区间的宽度</strong>，衡量不确定性，它是根据 Hoeffding 不等式推导而来：</li>
</ul>
<blockquote>
<p><strong>Hoeffding 不等式 </strong>：假设有 $n$ 个独立的、取值范围在 $[0,1]$ 的随机变量，那么它们的​平均值 $\hat{\mu}$ 与真实期望 $\mu$ 的偏差是有<strong> 上界 </strong> 的​：</p>
<script type="math/tex; mode=display">
\Pr\left(|\hat{\mu} - \mu| \ge \epsilon \right) \le 2 \exp(-2n \epsilon^2)</script><p>我们想找一个 <strong>置信区间宽度</strong> $\epsilon(n)$，使得真实期望 $\mu$ 落在以下区间的概率很高：</p>
<script type="math/tex; mode=display">
\Pr\left(\mu \le \hat{\mu} + \epsilon(n) \right) \ge 1 - \delta</script><p>等价地：</p>
<script type="math/tex; mode=display">
\Pr\left(\hat{\mu} + \epsilon(n) <\mu \right) \le \delta</script><p>根据 Hoeffding 不等式，我们只要让偏差上界等于 $\delta$：</p>
<script type="math/tex; mode=display">
\exp(-2n \epsilon^2) = \delta \quad\Rightarrow\quad \epsilon = \sqrt{\frac{ \ln(1/\delta) }{2n} }</script><p>UCB1 并不使用固定的置信水平 $\delta$，而是设置：</p>
<script type="math/tex; mode=display">
\delta = \frac{1}{t^4} \quad\Rightarrow\quad \ln \left(\frac{1}{\delta} \right) = 4 \ln t</script><p>这时置信区间变成：</p>
<script type="math/tex; mode=display">
\epsilon = \sqrt{\frac{4 \ln t}{2n} } = \sqrt{\frac{2 \ln t}{n} }</script><p>代入 $n = N_i(t-1)$，就得到了 UCB1 中使用的 <strong>探索项（置信区间宽度）</strong>：</p>
<script type="math/tex; mode=display">
\epsilon = \sqrt{\frac{2 \ln t}{N_i(t-1)} }</script></blockquote>
<p>UCB1 算法保证了前期探索，后期利用的整体策略，且累积遗憾是次线性的。UCB1 算法有严谨的上界分析，理论性能优秀。</p>
<h4 id="3- 汤普森采样（Thompson-Sampling）"><a href="#3- 汤普森采样（Thompson-Sampling）" class="headerlink" title="3. 汤普森采样（Thompson Sampling）"></a>3. 汤普森采样（Thompson Sampling）</h4><p>汤普森采样（Thompson Sampling）也称为概率匹配法（Probability Matching），其核心思想是利用贝叶斯推理，挑选​最大化后验概率分布期望的动作。</p>
<blockquote>
<p><strong>贝叶斯推理 </strong> 是统计学中基于贝叶斯定理的概率更新方法，其核心思想是通过新证据（数据）动态修正对未知参数的先验认知。</p>
<p><strong>贝叶斯定理 </strong> 的数学表达如下：</p>
<script type="math/tex; mode=display">
P(\theta \mid \text{data}) = \frac{P(\text{data} \mid \theta) \cdot P(\theta)}{P(\text{data})}</script><p>其中：</p>
<ul>
<li>$P(\theta)$：<strong>先验分布</strong>（Prior），表示在观测数据前对参数 $\theta$ 的初始认知（如历史经验或主观假设）。</li>
<li>$P(\text{data} \mid \theta)$：<strong>似然函数</strong>（Likelihood），描述在给定 $\theta$ 时，当前数据出现的概率。</li>
<li>$P(\theta \mid \text{data})$：<strong>后验分布</strong>（Posterior），整合数据后对 $\theta$ 的更新认知。</li>
<li>$P(\text{data})$：观测数据，即 <strong> 证据</strong>（Evidence），作为归一化常数，确保后验积分为 1。</li>
</ul>
<p>在贝叶斯统计中，如果后验分布与先验分布属于同一类分布族，则它们被称为 <strong> 共轭分布</strong>。</p>
</blockquote>
<p>使用汤普森采样求解 MAB 问题时，通常采用如下 <strong> 设定</strong>：</p>
<ol>
<li><p><strong>臂的奖励分布（似然分布）为伯努利分布</strong></p>
<ul>
<li>每个臂 $i$ （$i=1,\dots,K$）的真实未知成功概率记为 $p_i$。</li>
<li><p>每次拉动臂 $i$ 会得到一个二值奖励：</p>
<script type="math/tex; mode=display">
X_{i,t} \sim \mathrm{Bernoulli}(p_i),\quad X_{i,t} \in \{0,1\},\quad \Pr[X_{i,t}=1] = p_i, \Pr[X_{i,t}=0] = 1-p_i.</script></li>
</ul>
</li>
<li><p><strong>奖励分布中的参数分布（先验分布）为 Beta 分布</strong></p>
<ul>
<li>设定成功概率 $p_i$ 的先验分布为 Beta 分布：</li>
<li><p>通常若没有任何先验信息，可以取 $\alpha_i=\beta_i=1$，即 $\mathrm{Beta}(1,1)$（等价于 $p_i$ 为均匀分布）</p>
<script type="math/tex; mode=display">
p_i \sim \mathrm{Beta}(\alpha_i,\,\beta_i)</script></li>
</ul>
</li>
<li><p><strong>根据贝叶斯统计的共轭性，后验分布也为 Beta 分布</strong></p>
<ul>
<li>设每次对臂 $i$ 做一次拉动、观察到“成功”（奖励 $=1$）或“失败”（奖励 $=0$）后，都要更新对 $p_i$ 的后验分布。</li>
<li>假设到第 $t-1$ 轮为止，臂 $i$ 已被拉了 $S_i$ 次成功、$F_i$ 次失败（注意：这里 $S_i + F_i = N_i(t-1)$，即被选次数）。</li>
<li><p>如果先验 $p_i\sim\mathrm{Beta}(\alpha_i,\beta_i)$，且观测到 $S_i$ 次成功、$F_i$ 次失败，那么 $p_i$ 的后验分布依旧是一个 Beta 分布：</p>
<script type="math/tex; mode=display">
p_i \bigg| \{X_{i,s}\}_{s=1}^{t-1} \sim \mathrm{Beta}(\,\alpha_i + S_i,\beta_i + F_i)</script></li>
<li><p>换句话说，到第 $t-1$ 轮时，臂 $i$ 的后验分布参数为：</p>
<script type="math/tex; mode=display">
\alpha_i(t-1) = \alpha_i(0) + S_i, \quad\beta_i(t-1) = \beta_i(0) + F_i</script></li>
</ul>
</li>
</ol>
<p>根据汤普森采样， 第 $t$ 轮挑选臂的 <strong> 策略 </strong> 为：</p>
<ol>
<li><p><strong>对每个臂 $i$ 从其后验分布中随机抽一个样本</strong>：</p>
<script type="math/tex; mode=display">
\theta_i \sim \mathrm{Beta}\bigl(\alpha_i(t-1),\beta_i(t-1)\bigr)</script><p>这里 $\theta_i$ 可以看作“根据到目前为止的信息，臂 $i$ 的潜在真实成功率”的一个随机猜测。</p>
</li>
<li><p><strong>根据抽样值 $\{\theta_i\}_{i=1}^K$ 选臂</strong>：</p>
<script type="math/tex; mode=display">
A_t = \arg\max_{1 \le i \le K} \bigl\{\theta_i\bigr\}</script><p>换句话说，实际选择那个“这次恰好随机抽得最大 $\theta_i$”的臂。</p>
</li>
<li><p><strong>拉动所选臂 $A_t$，观察到奖励</strong> $X_{A_t,t} \in \{0,1\}$。</p>
<ul>
<li>若 $X_{A_t,t} = 1$，说明成功一次，于是更新该臂的 “成功计数” $S_{A_t} \leftarrow S_{A_t} + 1$，对应后验参数 $\alpha_{A_t} \leftarrow \alpha_{A_t} + 1$。</li>
<li>若 $X_{A_t,t} = 0$，说明失败一次，于是更新该臂的 “失败计数” $F_{A_t} \leftarrow F_{A_t} + 1$，对应后验参数 $\beta_{A_t} \leftarrow \beta_{A_t} + 1$。</li>
</ul>
</li>
<li>重复上述步骤到第 $T$ 轮结束。</li>
</ol>
<p>随着轮次 $t$ 的增加，我们不断更新后验分布，每个臂 $i$ 的后验分布 $\mathrm{Beta}(\alpha_i(t-1),\beta_i(t-1))$ 越来越集中，其均值 $\frac{\alpha_i(t-1)}{\alpha_i(t-1) + \beta_i(t-1)}$ 也会越来越接近真实值 $p_i$ 。这时越有可能是最优的臂，它被抽到的概率就越大，充分利用了已有信息；但同时由于不确定性，偶尔也会有次优但尚未充分探索的臂被选到，以此积累更多信息。</p>
<img src="/2025/%E5%A4%9A%E8%87%82%E8%80%81%E8%99%8E%E6%9C%BA/beta%E5%88%86%E5%B8%83.png" class title="beta 分布图 ">
<p>汤普森采样从贝叶斯视角自然地平衡探索与利用，累积遗憾是次线性的，在实践中表现往往优于 UCB1，且可以扩展到非伯努利奖励、多维上下文等更复杂的场景。</p>
<h2 id="二、多臂老虎机变体问题"><a href="# 二、多臂老虎机变体问题" class="headerlink" title="二、多臂老虎机变体问题"></a>二、多臂老虎机变体问题 </h2><h3 id="（一）上下文多臂老虎机（Contextual-Bandits）"><a href="#（一）上下文多臂老虎机（Contextual-Bandits）" class="headerlink" title="（一）上下文多臂老虎机（Contextual Bandits）"></a>（一）上下文多臂老虎机（Contextual Bandits）</h3><p> 每一轮决策前都会收到一个上下文（context）向量 $x_t \in \mathbb{R}^d$，它描述了当前时刻的环境信息。奖励不仅取决于选择的臂，还与上下文相关。</p>
<p>目标是设计一个策略 $\pi(x_t)$，使得在上下文 $x_t$ 出现时选臂 $A_t = \pi(x_t)$，从而最大化累计期望奖励 $\sum_{t=1}^T \mathbb{E}[X_{A_t,t}]$。</p>
<h4 id="LinUCB- 算法"><a href="#LinUCB- 算法" class="headerlink" title="LinUCB 算法"></a><strong>LinUCB 算法 </strong></h4><p> 假设与上下文相关的奖励服从线性回归模型：</p>
<script type="math/tex; mode=display">
\mathbb{E}\bigl[X_{i,t} \,\bigm|\, x_t\bigr] = x_t^\top \theta_i</script><p>其中，$\theta_i \in \mathbb{R}^d$ 是臂 $i$ 的未知参数向量，用来衡量在不同上下文下，臂 $i$ 的期望奖励。</p>
<p>LinUCB 假设对每个臂 $i$ 都做一个带正则化的线性回归，用历史数据来估计 $\theta_i$。具体步骤如下：</p>
<ol>
<li><p><strong>维护矩阵和向量 </strong><br> 对臂 $i$，设它从开始到目前被选过 $n_i$ 次，所对应的上下文矩阵为：</p>
<script type="math/tex; mode=display">
D_i
  =
  \begin{bmatrix}
    x_{i,1}^\top \\ 
    x_{i,2}^\top \\ 
    \vdots \\
    x_{i,n_i}^\top
  \end{bmatrix}
  \in\mathbb{R}^{\,n_i \times d}</script><p>其中第 $j$ 行 $x_{i,j}^\top$ 就是第 $j$ 次选了臂 $i$ 时的上下文。<br>同时，把对应的历史奖励（随机变量）记作向量：</p>
<script type="math/tex; mode=display">
Y_i =
  \begin{bmatrix}
    X_{i,1} \\ 
    X_{i,2} \\ 
    \vdots \\ 
    X_{i,n_i}
  \end{bmatrix}
  \in\mathbb{R}^{\,n_i}</script></li>
<li><p><strong>构造协方差矩阵 $A_i$ 和估计 $\hat{\theta}_i$</strong><br>引入正则化系数 $\lambda &gt; 0$，定义：</p>
<script type="math/tex; mode=display">
A_i = D_i^\top D_i + \lambda I_d</script><p>这其中 $D_i^\top D_i$ 是普通最小二乘所用的 “Gram 矩阵”，$\lambda I_d$ 用来保证可逆并防止过拟合。<br>$\theta_i$ 对应的回归估计为：</p>
<script type="math/tex; mode=display">
\hat{\theta}_i = A_i^{-1}D_i^ \top Y_i</script><p>那么在第 $t$ 轮到来且上下文为 $x_t$ 时，臂 $i$ 的奖励估计就是 $x_t^\top \hat{\theta}_i$。</p>
</li>
<li><p><strong>计算上置信界并选择臂</strong><br>LinUCB 会为每个臂 $i$ 计算一个上置信界（Upper Confidence Bound）：</p>
<script type="math/tex; mode=display">
\text{UCB}_i(t) = x_t^\top \hat{\theta}_i  + \alpha \sqrt{x_t^\top A_i^{-1} x_t},</script><p>这里：</p>
<ul>
<li>$x_t^\top \hat{\theta}_i$ 就是“基于回归模型的当前估计均值”（利用）；</li>
<li>$\sqrt{x_t^\top A_i^{-1} x_t}$ 表示“在这个 $x_t$ 方向上的估计不确定度”（探索），如果臂 $i$ 过去在相似上下文下数据不够，$A_i^{-1}$ 在对应方向就比较大。乘上调节系数 $\alpha$（一般取稍大，比如 1 或 2），可以给不确定的臂更多探索的空间。</li>
</ul>
<p>最终选择“奖励估计 + 探索补偿”最大的臂为最优臂：</p>
<script type="math/tex; mode=display">
A_t = \arg\max_{1 \le i \le K}\Bigl\{x_t^\top \hat{\theta}_i + \alpha\sqrt{x_t^\top A_i^{-1} x_t}\Bigr\}</script></li>
<li><p><strong>观测奖励并更新 </strong><br> 选到臂 $A_t$ 后，拉动它得到真实奖励 $X_{A_t,t}$，再把这一对 $\bigl(x_t, X_{A_t,t}\bigr)$ 加入对应的 $(D_{A_t}, Y_{A_t})$ 中，更新：</p>
<script type="math/tex; mode=display">
D_{A_t} \leftarrow 
  \begin{bmatrix} 
    D_{A_t} \\[4pt] x_t^\top 
  \end{bmatrix},\qquad
Y_{A_t} \leftarrow 
  \begin{bmatrix} 
    Y_{A_t} \\[4pt] X_{A_t,t} 
  \end{bmatrix},</script><p>然后重新计算：</p>
<script type="math/tex; mode=display">
A_{A_t} = D_{A_t}^\top D_{A_t} + \lambda I,\quad
\hat{\theta}_{A_t} = A_{A_t}^{-1} \, D_{A_t}^\top \, Y_{A_t}.</script><p>这样下一轮到来时，模型对臂 $A_t$ 的 $\theta_{A_t}$ 估计就会更准确。</p>
</li>
</ol>
<h3 id="（二）非平稳 -MAB（Non-Stationary-Bandits）"><a href="#（二）非平稳 -MAB（Non-Stationary-Bandits）" class="headerlink" title="（二）非平稳 MAB（Non-Stationary Bandits）"></a>（二）非平稳 MAB（Non-Stationary Bandits）</h3><p>在经典 MAB 中，我们假设每个臂 $i$ 的期望奖励 $\mu_i$ 是固定不变的。但在很多实际场景下（比如用户偏好随时间变化、设备老化导致性能波动等），臂的真实期望 $\mu_i(t)$ 会随着时间 $t$ 改变。这时如果继续用固定环境下的算法，往往会把过去积累的大量“旧数据”当作对当前环境的有效信息，而没有及时跟“新现象”对齐，决策出现严重劣化。</p>
<p>非平稳 MAB 的核心需求就是：<strong>策略能够适应这类“随时间漂移”的奖励分布</strong>，对古老数据加以“遗忘”或“衰减”，让自己的决策更多依赖于最近的观察。</p>
<ul>
<li>$X_{i,t}$：第 $t$ 轮拉臂 $i$ 时得到的随机奖励。</li>
<li>$A_t$：第 $t$ 轮选中的臂编号。</li>
<li>$\hat{\mu}_i(t)$：针对臂 $i$ 在第 $t$ 轮的当前平均奖励估计。</li>
</ul>
<h4 id="1- 滑动窗口 -UCB（SW-UCB）"><a href="#1- 滑动窗口 -UCB（SW-UCB）" class="headerlink" title="1. 滑动窗口 UCB（SW-UCB）"></a>1. 滑动窗口 UCB（SW-UCB）</h4><p><strong>思路</strong>：只用最近 $w$ 轮得到的数据来估计每个臂的当前平均和置信区间，舍弃更早数据。这样可以在环境突变时快速“忘掉”过时信息。</p>
<ol>
<li><p><strong>窗口内次数统计 </strong><br> 定义滑动窗口（最近 $w$ 轮的索引集合）：</p>
<script type="math/tex; mode=display">
\mathcal{W}_t = \{s : t - w + 1 \le s \le t\}</script><p>对臂 $i$ 而言，记在窗口 $\mathcal{W}_t$ 中被选择的总次数为：</p>
<script type="math/tex; mode=display">
N_i^{\mathrm{win}}(t) = \sum_{s = t - w + 1}^{t} \mathbb{I}\{A_s = i\}</script></li>
<li><p><strong>滑动窗口内平均奖励 </strong><br> 在第 $t$ 轮，要估计臂 $i$ 的当前平均奖励，只使用最近 $w$ 轮拉该臂时观察到的奖励：</p>
<script type="math/tex; mode=display">
\hat{\mu}_i(t)
=
\begin{cases}
  \displaystyle
  \frac{1}{N_i^{\mathrm{win}}(t)}
  \sum_{s = t - w + 1}^{t} X_{i,s} \mathbb{I}\{A_s = i\}, 
  & N_i^{\mathrm{win}}(t) > 0, \\[8pt]
  0, & N_i^{\mathrm{win}}(t) = 0,
\end{cases}</script><p>其中若 $N_i^{\mathrm{win}}(t)=0$ 表示在窗口期内尚未拉过臂 $i$，可先将 $\hat{\mu}_i(t)$ 设为 0 或者一个默认值，以保证算法会去探索它一次。</p>
</li>
<li><p><strong>滑动窗口 UCB</strong><br>类似常规 UCB，只不过“奖励估计 + 置信区间”都限制在窗口内数据：</p>
<script type="math/tex; mode=display">
\mathrm{SW\text{-}UCB}_i(t) = \hat{\mu}_i(t) + c \sqrt{\frac{\ln t}{N_i^{\mathrm{win}}(t)}}</script><p>其中：</p>
<ul>
<li>$\displaystyle \sqrt{\frac{\ln t}{N_i^{\mathrm{win}}(t)}}$ 是基于窗口内样本数的置信宽度；</li>
<li>常数 $c&gt;0$ 用来调节探索强度；</li>
<li>如果在窗口内 $N_i^{\mathrm{win}}(t)=0$（尚无样本），通常将该臂的 UCB 视为 $+\infty$，以保证它至少被试一次。</li>
</ul>
</li>
<li><p><strong>选择与更新 </strong><br> 第 $t$ 轮到来时，先计算每个臂的 $\mathrm{SW\text{-}UCB}_i(t)$，然后选择：</p>
<script type="math/tex; mode=display">
A_t = \arg\max_{1\le i\le K}\mathrm{SW\text{-}UCB}_i(t)</script><p>拉臂 $A_t$，观测奖励 $X_{A_t,t}$ 并存入时间戳 $t$；<br>下一轮 $t+1$ 到来时，窗口滑动为 $\{t - w + 2,\,\dots,\,t+1\}$，重新统计新的 $N_i^{\mathrm{win}}(t+1)$ 和 $\hat{\mu}_i(t+1)$，如此循环。</p>
</li>
</ol>
<p>通过只看最近 (w) 轮的数据，SW-UCB 能够丢弃旧分布的统计，快速适应新分布。但窗口大小 (w) 的选取非常重要：窗口太小，数据不够，估计方差大；窗口太大，忘却慢，适应性差。 </p>
<h4 id="方法二：指数加权平均（EXP3-S）"><a href="# 方法二：指数加权平均（EXP3-S）" class="headerlink" title="方法二：指数加权平均（EXP3.S）"></a>方法二：指数加权平均（EXP3.S）</h4><p><strong>思路</strong>：给每个臂维护一个权重，用它来计算各臂的概率分布；每轮得到的奖励不仅立即用于更新，还会被“指数衰减”以快速忘却旧数据。</p>
<ol>
<li><strong>初始化 </strong><br> 对每个臂 $i=1,\dots,K$，初始化一个权重 $w_i(1) = 1$。<br>设定衰减因子 $\rho \in (0,1)$（如 $0.99$），学习率 $\eta &gt; 0$，探索参数 $\gamma \in (0,1)$。</li>
<li><p><strong>每轮计算概率 </strong><br> 在第 $t$ 轮，根据当前权重计算选臂概率向量 $\{p_i(t)\}$：</p>
<script type="math/tex; mode=display">
p_i(t) = (1 - \gamma)\frac{w_i(t)}{\sum_{j=1}^K w_j(t)} + \frac{\gamma}{K}, \quad i = 1,\dots,K.</script><p>其中 $\frac{w_i(t)}{\sum_j w_j(t)}$ 是“利用”部分，让高权重臂被选概率大；$\gamma/K$ 保证全局随机“探索”。</p>
</li>
<li><strong>抽样与拉臂 </strong><br> 按 $\{p_i(t)\}$ 的概率分布随机选择一个臂 $A_t$，拉一次并观测奖励 $X_{A_t,t}$。</li>
<li><p><strong>权重的指数更新（带衰减）</strong><br>首先，对臂 $i$ 构造加权奖励估计：</p>
<script type="math/tex; mode=display">
\widehat{X}_{i,t}
=
\begin{cases}
  \displaystyle
  \frac{X_{i,t}}{p_i(t)}, 
  & \text{if} i = A_t, \\[8pt]
  0, & \text{if} i \neq A_t.
\end{cases}</script><p>然后，对所有臂先做“指数衰减”：</p>
<script type="math/tex; mode=display">
w_i(t) \longleftarrow \rho w_i(t), \quad i=1,\dots,K</script><p>再针对被选臂 $A_t$ 用当前的“加权奖励” $\widehat{X}_{A_t,t}$ 做指数更新：</p>
<script type="math/tex; mode=display">
w_{A_t}(t+1) = w_{A_t}(t) \exp\Bigl(\eta \widehat{X}_{A_t,t}\Bigr)</script><p>其余 $i \neq A_t$ 的权重在仅做“衰减”后，直接形成下一轮权重：</p>
<script type="math/tex; mode=display">
w_i(t+1) = w_i(t), \quad i \neq A_t</script></li>
<li><strong>第 $(t+1)$ 轮继续 </strong><br> 更新后权重 $\{w_i(t+1)\}$，下一轮按照相同步骤计算新的 $\{p_i(t+1)\}$，继续抽样拉臂。</li>
</ol>
<p>EXP3.S 通过“指数衰减”快速遗忘旧数据，又通过“指数增益”放大近期表现好的臂，能够在非平稳或对抗环境下迅速调整，保持算法对最新变化的敏感性。</p>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/MAB/">MAB</a><a class="post-meta__tags" href="/tags/UCB/">UCB</a></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related full-width" href="/2025/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1%E6%9C%BA%E5%88%B6/" title="进程间通信机制"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">Previous</div><div class="info-item-2">进程间通信机制</div></div><div class="info-2"><div class="info-item-1">进程间通信机制包括管道（匿名/命名）、共享内存、套接字和消息队列，分别适用于亲缘进程通信、大数据共享、跨主机通信及异步任务处理。</div></div></div></a></nav><hr class="custom-hr"><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comments</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/photo.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"></div><div class="author-info-name">一瓢清浅</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">34</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">37</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">18</div></a></div><a id="card-info-btn" target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/jiliguluss"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">个人技术博客，涉及软件开发、安全测试、AI算法等领域。不求精深，但当涉猎，有所闻耳。</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E5%A4%9A%E8%87%82%E8%80%81%E8%99%8E%E6%9C%BA%E9%97%AE%E9%A2%98"><span class="toc-text">一、多臂老虎机问题 </span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%88%E4%B8%80%EF%BC%89%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1"><span class="toc-text">（一）数学建模 </span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%88%E4%BA%8C%EF%BC%89%E6%B1%82%E8%A7%A3%E7%AE%97%E6%B3%95"><span class="toc-text">（二）求解算法 </span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-varepsilon-%20%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95%EF%BC%88-varepsilon-Greedy%EF%BC%89"><span class="toc-text">1. $\varepsilon$- 贪心算法（$\varepsilon$-Greedy）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%20%E4%B8%8A%E7%BD%AE%E4%BF%A1%E7%95%8C%E7%AE%97%E6%B3%95%EF%BC%88UCB1%EF%BC%89"><span class="toc-text">2. 上置信界算法（UCB1）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%20%E6%B1%A4%E6%99%AE%E6%A3%AE%E9%87%87%E6%A0%B7%EF%BC%88Thompson-Sampling%EF%BC%89"><span class="toc-text">3. 汤普森采样（Thompson Sampling）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E5%A4%9A%E8%87%82%E8%80%81%E8%99%8E%E6%9C%BA%E5%8F%98%E4%BD%93%E9%97%AE%E9%A2%98"><span class="toc-text">二、多臂老虎机变体问题 </span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%88%E4%B8%80%EF%BC%89%E4%B8%8A%E4%B8%8B%E6%96%87%E5%A4%9A%E8%87%82%E8%80%81%E8%99%8E%E6%9C%BA%EF%BC%88Contextual-Bandits%EF%BC%89"><span class="toc-text">（一）上下文多臂老虎机（Contextual Bandits）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#LinUCB-%20%E7%AE%97%E6%B3%95"><span class="toc-text">LinUCB 算法 </span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%88%E4%BA%8C%EF%BC%89%E9%9D%9E%E5%B9%B3%E7%A8%B3%20-MAB%EF%BC%88Non-Stationary-Bandits%EF%BC%89"><span class="toc-text">（二）非平稳 MAB（Non-Stationary Bandits）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%20%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%20-UCB%EF%BC%88SW-UCB%EF%BC%89"><span class="toc-text">1. 滑动窗口 UCB（SW-UCB）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E6%B3%95%E4%BA%8C%EF%BC%9A%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87%EF%BC%88EXP3-S%EF%BC%89"><span class="toc-text">方法二：指数加权平均（EXP3.S）</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/%E5%A4%9A%E8%87%82%E8%80%81%E8%99%8E%E6%9C%BA/" title="多臂老虎机">多臂老虎机</a><time datetime="2025-05-28T00:57:38.000Z" title="Created 2025-05-28 08:57:38">2025-05-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1%E6%9C%BA%E5%88%B6/" title="进程间通信机制">进程间通信机制</a><time datetime="2025-05-26T06:24:56.000Z" title="Created 2025-05-26 14:24:56">2025-05-26</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/Web%E5%BA%94%E7%94%A8%E6%9E%B6%E6%9E%84/" title="Web 应用架构">Web 应用架构</a><time datetime="2025-05-21T06:52:47.000Z" title="Created 2025-05-21 14:52:47">2025-05-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/%E3%80%8A%E8%82%A1%E7%A5%A8%E6%8A%95%E8%B5%84%E5%85%A5%E9%97%A8%E3%80%81%E8%BF%9B%E9%98%B6%E4%B8%8E%E5%AE%9E%E6%88%98%E3%80%8B%E7%AC%94%E8%AE%B0/" title="《股票投资入门、进阶与实战》笔记">《股票投资入门、进阶与实战》笔记</a><time datetime="2025-02-24T04:50:18.000Z" title="Created 2025-02-24 12:50:18">2025-02-24</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/%E3%80%8A%E6%8A%95%E8%B5%84%E4%B8%AD%E6%9C%80%E7%AE%80%E5%8D%95%E7%9A%84%E4%BA%8B%E3%80%8B%E7%AC%94%E8%AE%B0/" title="《投资中最简单的事》笔记">《投资中最简单的事》笔记</a><time datetime="2025-01-20T01:05:49.000Z" title="Created 2025-01-20 09:05:49">2025-01-20</time></div></div></div></div></div></div></main><footer id="footer" style="background: transparent;"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2025 By 一瓢清浅</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="Scroll to Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = `%%{init:{ 'theme':'${theme}'}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (true) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>(() => {
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const commentCount = n => {
    const isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
    if (isCommentCount) {
      isCommentCount.textContent= n
    }
  }

  const initGitalk = (el, path) => {
    if (isShuoshuo) {
      window.shuoshuoComment.destroyGitalk = () => {
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }

    const gitalk = new Gitalk({
      clientID: 'Ov23li8lUWNadcQIsjjS',
      clientSecret: 'a3a90b60503b4f298c2a09778e26da53ae01f9cd',
      repo: 'comment',
      owner: 'jiliguluss',
      admin: ['jiliguluss'],
      updateCountCallback: commentCount,
      ...option,
      id: isShuoshuo ? path : (option && option.id) || 'c5b2baedab12c33d6cd85bd1157de143'
    })

    gitalk.render('gitalk-container')
  }

  const loadGitalk = async(el, path) => {
    if (typeof Gitalk === 'function') initGitalk(el, path)
    else {
      await btf.getCSS('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css')
      await btf.getScript('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.js')
      initGitalk(el, path)
    }
  }

  if (isShuoshuo) {
    'Gitalk' === 'Gitalk'
      ? window.shuoshuoComment = { loadComment: loadGitalk }
      : window.loadOtherComment = loadGitalk
    return
  }

  if ('Gitalk' === 'Gitalk' || !false) {
    if (false) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
    else loadGitalk()
  } else {
    window.loadOtherComment = loadGitalk
  }
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>